#################################################################################################################
# Create an object store with settings for replication in a production environment. A minimum of 3 hosts with
# OSDs are required in this example.
#  kubectl create -f object.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: ocs-storagecluster-cephobjectstore
  namespace: rook-ceph # namespace:cluster
spec:
  dataPool:
    deviceClass: ssd
    enableCrushUpdates: true
    failureDomain: host
    replicated:
      replicasPerFailureDomain: 1
      size: 3
      targetSizeRatio: 0.49
  gateway:
    instances: 1
    labels:
      odf-resource-profile: ""
    placement:
      #nodeAffinity:
      #  requiredDuringSchedulingIgnoredDuringExecution:
      #    nodeSelectorTerms:
      #    - matchExpressions:
      #      - key: cluster.ocs.openshift.io/openshift-storage
      #        operator: Exists
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-rgw
              topologyKey: kubernetes.io/hostname
            weight: 100
      tolerations:
        - effect: NoSchedule
          key: node.ocs.openshift.io/storage
          operator: Equal
          value: "true"
    port: 80
    priorityClassName: openshift-user-critical
    readAffinity:
      type: localize
    resources:
      limits:
        cpu: "1"
        memory: 2Gi
      requests:
        cpu: "1"
        memory: 2Gi
    securePort: 443
    service:
      annotations:
        service.beta.openshift.io/serving-cert-secret-name: ocs-storagecluster-cos-ceph-rgw-tls-cert
  metadataPool:
    deviceClass: ssd
    enableCrushUpdates: true
    failureDomain: host
    replicated:
      replicasPerFailureDomain: 1
      size: 3
  protocols:
    swift: null
  sharedPools:
    preserveRadosNamespaceDataOnDelete: false
